{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NK2mfY_UhC9"
   },
   "source": [
    "======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dP2-yiHMC23Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Installing Egyptian ID OCR - NumPy Fixed Version\n",
      "======================================================================\n",
      "\n",
      "ğŸ”§ Step 1/4: Fixing NumPy compatibility issue...\n",
      "   (Downgrading NumPy 2.4.1 â†’ NumPy 1.26.4)\n",
      "âœ… NumPy 1.26.4 installed\n",
      "\n",
      "ğŸ“¦ Step 2/4: Installing core packages...\n",
      "âœ… ultralytics\n",
      "âœ… roboflow\n",
      "âœ… arabic-reshaper\n",
      "âœ… python-bidi\n",
      "âœ… openpyxl\n",
      "\n",
      "ğŸ“¦ Step 3/4: Installing PaddleOCR...\n",
      "âœ… paddlepaddle-gpu 2.6.1\n",
      "âœ… paddleocr 2.7.3\n",
      "\n",
      "ğŸ“¦ Step 4/4: Locking NumPy version...\n",
      "âœ… NumPy locked at 1.26.4\n",
      "\n",
      "======================================================================\n",
      "âœ… INSTALLATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "âš ï¸  CRITICAL: You MUST restart runtime now!\n",
      "\n",
      "ğŸ“ Next steps:\n",
      "   1. Runtime â†’ Restart runtime\n",
      "   2. Run the test cell to verify\n",
      "   3. Do NOT run any other cells until after restart\n",
      "\n",
      "ğŸ’¡ After restart, NumPy will stay at 1.26.4 and everything will work\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EGYPTIAN ID OCR - NUMPY FIX FOR GOOGLE COLAB\n",
    "# ============================================================================\n",
    "# This fixes the NumPy 2.x compatibility issue\n",
    "# Run this cell, then RESTART RUNTIME\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸš€ Installing Egyptian ID OCR - NumPy Fixed Version\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â”€â”€â”€ CRITICAL FIX: Downgrade NumPy FIRST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ”§ Step 1/4: Fixing NumPy compatibility issue...\")\n",
    "print(\"   (Downgrading NumPy 2.4.1 â†’ NumPy 1.26.4)\")\n",
    "\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"numpy==1.26.4\", \"--force-reinstall\"],\n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "print(\"âœ… NumPy 1.26.4 installed\")\n",
    "\n",
    "# â”€â”€â”€ STEP 2: Install core packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“¦ Step 2/4: Installing core packages...\")\n",
    "\n",
    "core_packages = [\n",
    "    \"ultralytics\",\n",
    "    \"roboflow\",\n",
    "    \"arabic-reshaper\",\n",
    "    \"python-bidi\",\n",
    "    \"openpyxl\",\n",
    "]\n",
    "\n",
    "for pkg in core_packages:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
    "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print(f\"âœ… {pkg}\")\n",
    "\n",
    "# â”€â”€â”€ STEP 3: Install PaddleOCR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“¦ Step 3/4: Installing PaddleOCR...\")\n",
    "\n",
    "# Install compatible PaddlePaddle version\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"paddlepaddle-gpu==2.6.1\"],\n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "print(\"âœ… paddlepaddle-gpu 2.6.1\")\n",
    "\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"paddleocr==2.7.3\"],\n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "print(\"âœ… paddleocr 2.7.3\")\n",
    "\n",
    "# â”€â”€â”€ STEP 4: Pin NumPy to prevent auto-upgrade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“¦ Step 4/4: Locking NumPy version...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"numpy==1.26.4\", \"--no-deps\"],\n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "print(\"âœ… NumPy locked at 1.26.4\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… INSTALLATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nâš ï¸  CRITICAL: You MUST restart runtime now!\")\n",
    "print(\"\\nğŸ“ Next steps:\")\n",
    "print(\"   1. Runtime â†’ Restart runtime\")\n",
    "print(\"   2. Run the test cell to verify\")\n",
    "print(\"   3. Do NOT run any other cells until after restart\")\n",
    "print(\"\\nğŸ’¡ After restart, NumPy will stay at 1.26.4 and everything will work\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIsKP2yJ7Dhl"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CMoiZqGmya9V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "download https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/Multilingual_PP-OCRv3_det_infer.tar to /home/marno000onaaa/.paddleocr/whl/det/ml/Multilingual_PP-OCRv3_det_infer/Multilingual_PP-OCRv3_det_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.85M/3.85M [00:36<00:00, 106kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/arabic_PP-OCRv4_rec_infer.tar to /home/marno000onaaa/.paddleocr/whl/rec/arabic/arabic_PP-OCRv4_rec_infer/arabic_PP-OCRv4_rec_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.92M/7.92M [00:34<00:00, 229kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to /home/marno000onaaa/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.19M/2.19M [00:23<00:00, 91.7kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PaddleOCR ready\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)  # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† 1.26.4\n",
    "\n",
    "from paddleocr import PaddleOCR\n",
    "ocr = PaddleOCR(lang='ar', use_angle_cls=True, use_gpu=False, show_log=False)\n",
    "print(\"âœ… PaddleOCR ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PafS1lswUb5p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports done!\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json, re, yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "print(\"âœ… Imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n",
      "12.8\n",
      "Quadro M1200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "V168Pmn4Xgnz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project directory created at:\n",
      "OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "PROJECT_DIR = r\"OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-\"\n",
    "\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "\n",
    "for d in ['models', 'outputs', 'results', 'dataset']:\n",
    "    os.makedirs(os.path.join(PROJECT_DIR, d), exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Project directory created at:\\n{PROJECT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Qzy2DOfyXlUd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in IDS-4 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15423/15423 [00:09<00:00, 1616.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to IDS-4 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 562/562 [00:00<00:00, 3502.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset downloaded to:\n",
      "/home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/IDS-4\n",
      "ğŸ“Š Classes:\n",
      "['address', 'birth_date', 'country', 'full_name', 'id_number']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "API_KEY = \"SNKUr0sNWb9nWYv1KO5R\"\n",
    "WORKSPACE = \"mywork-f0ue6\"\n",
    "PROJECT = \"ids-z3sti\"\n",
    "VERSION = 4\n",
    "\n",
    "DATASET_DIR = os.path.join(PROJECT_DIR, \"dataset\")\n",
    "\n",
    "# ===============================\n",
    "# Change working directory\n",
    "# ===============================\n",
    "os.chdir(DATASET_DIR)\n",
    "\n",
    "# ===============================\n",
    "# Download dataset\n",
    "# ===============================\n",
    "rf = Roboflow(api_key=API_KEY)\n",
    "project = rf.workspace(WORKSPACE).project(PROJECT)\n",
    "dataset = project.version(VERSION).download(\"yolov8\")\n",
    "\n",
    "# ===============================\n",
    "# Load YAML\n",
    "# ===============================\n",
    "data_yaml_path = os.path.join(dataset.location, \"data.yaml\")\n",
    "\n",
    "with open(data_yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ… Dataset downloaded to:\")\n",
    "print(dataset.location)\n",
    "\n",
    "print(\"ğŸ“Š Classes:\")\n",
    "print(config[\"names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "zfOABXbjqxzR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using existing model\n",
      "âœ… Model ready at: /home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models/best.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "\n",
    "PROJECT_MODELS_DIR = \"/home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models\"\n",
    "os.makedirs(PROJECT_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(PROJECT_MODELS_DIR, \"best.pt\")\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"âœ… Using existing model\")\n",
    "    model = YOLO(MODEL_PATH)\n",
    "else:\n",
    "    print(\"ğŸš€ Training YOLOv8...\")\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    model.train(\n",
    "        data=\"/home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/data.yaml\",\n",
    "        epochs=150,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        device=0,\n",
    "        patience=20,\n",
    "        project=PROJECT_MODELS_DIR,\n",
    "        name='detector',\n",
    "        augment=True,\n",
    "        hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,\n",
    "        degrees=5, translate=0.1, scale=0.2\n",
    "    )\n",
    "\n",
    "    # Ù†Ø³Ø® Ø£ÙØ¶Ù„ ÙˆØ²Ù† Ù…Ø¨Ø§Ø´Ø±Ø© Ù‡Ù†Ø§\n",
    "    best_weight = os.path.join(PROJECT_MODELS_DIR, \"detector/weights/best.pt\")\n",
    "    shutil.copy(best_weight, MODEL_PATH)\n",
    "    model = YOLO(MODEL_PATH)\n",
    "\n",
    "print(\"âœ… Model ready at:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Training YOLOv8 on CPU...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 1.9MB/s 3.4s.3s<0.1s2s8ss\n",
      "Ultralytics 8.4.7 ğŸš€ Python-3.12.3 torch-2.9.1+cu128 CPU (Intel Core i7-6820HQ 2.70GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/IDS-4/data.yaml, degrees=5, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=150, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=detector6, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/runs/detect/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models/detector6, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.2, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, 16, None, [64, 128, 256]] \n",
      "Model summary: 130 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1055.5Â±356.9 MB/s, size: 43.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/IDS-4/train/labels... 243 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 243/243 1.5Kit/s 0.2s<0.3s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/IDS-4/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1395.3Â±452.7 MB/s, size: 46.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/IDS-4/valid/labels... 12 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 12/12 1.3Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/dataset/IDS-4/valid/labels.cache\n",
      "Plotting labels to /home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/runs/detect/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models/detector6/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/marno000onaaa/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/runs/detect/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models/detector6\u001b[0m\n",
      "Starting training for 150 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/150         0G      2.742      3.946      2.564        114        640: 6% â•¸â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1/16 30.4s/it 20.9s<7:35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸš€ Training YOLOv8 on CPU...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/data.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# â† Ù‡Ù†Ø§ CPU Ø¨Ø¯Ù„ GPU\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROJECT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/models\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdetector\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhsv_h\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.015\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhsv_s\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhsv_v\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshutil\u001b[39;00m\n\u001b[32m     27\u001b[39m best = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/models/detector/weights/best.pt\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:774\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    772\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:244\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:440\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    437\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = \u001b[38;5;28mself\u001b[39m.loss_items \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m)\n\u001b[32m    439\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL_PATH = f'{PROJECT_DIR}/models/best.pt'\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"âœ… Using existing model\")\n",
    "    model = YOLO(MODEL_PATH)\n",
    "else:\n",
    "    print(\"ğŸš€ Training YOLOv8 on CPU...\")\n",
    "    model = YOLO('yolov8n.pt')\n",
    "\n",
    "    model.train(\n",
    "        data=f'{dataset.location}/data.yaml',\n",
    "        epochs=150,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        device='cpu',       # â† Ù‡Ù†Ø§ CPU Ø¨Ø¯Ù„ GPU\n",
    "        patience=20,\n",
    "        project=f'{PROJECT_DIR}/models',\n",
    "        name='detector',\n",
    "        augment=True,\n",
    "        hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,\n",
    "        degrees=5, translate=0.1, scale=0.2\n",
    "    )\n",
    "\n",
    "    import shutil\n",
    "    best = f'{PROJECT_DIR}/models/detector/weights/best.pt'\n",
    "    shutil.copy(best, MODEL_PATH)\n",
    "    model = YOLO(MODEL_PATH)\n",
    "\n",
    "print(\"âœ… Model ready!\")\n",
    "\n",
    "# â”€â”€â”€ EVALUATE\n",
    "metrics = model.val(device='cpu')   # â† CPU Ù‡Ù†Ø§ ÙƒÙ…Ø§Ù†\n",
    "\n",
    "print(f\"\\nğŸ“Š mAP@50: {metrics.box.map50:.3f}\")\n",
    "print(f\"ğŸ“Š Precision: {metrics.box.mp:.3f}\")\n",
    "print(f\"ğŸ“Š Recall: {metrics.box.mr:.3f}\\n\")\n",
    "\n",
    "for i, name in enumerate(config['names']):\n",
    "    if i < len(metrics.box.ap50):\n",
    "        print(f\"  {name:15s}: {metrics.box.ap50[i]:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "8-n-VbvRXm5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.7 ğŸš€ Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (Quadro M1200, 4038MiB)\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# â”€â”€â”€ CELL 6: EVALUATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m metrics = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“Š mAP@50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.box.map50\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.box.mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:612\u001b[39m, in \u001b[36mModel.val\u001b[39m\u001b[34m(self, validator, **kwargs)\u001b[39m\n\u001b[32m    609\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m    611\u001b[39m validator = (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m))(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = validator.metrics\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m validator.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/engine/validator.py:159\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    157\u001b[39m     LOGGER.warning(\u001b[33m\"\u001b[39m\u001b[33mvalidating an untrained model YAML will result in 0 mAP.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m callbacks.add_integration_callbacks(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m model = \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRANK\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRANK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28mself\u001b[39m.device = model.device  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mself\u001b[39m.args.half = model.fp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:208\u001b[39m, in \u001b[36mAutoBackend.__init__\u001b[39m\u001b[34m(self, model, device, dnn, data, fp16, fuse, verbose)\u001b[39m\n\u001b[32m    206\u001b[39m             model = model.to(device)\n\u001b[32m    207\u001b[39m         model = model.fuse(verbose=verbose)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pt file\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_checkpoint\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:286\u001b[39m, in \u001b[36mBaseModel._apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[32m    278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply a function to all tensors in the model that are not parameters or registered buffers.\u001b[39;00m\n\u001b[32m    279\u001b[39m \n\u001b[32m    280\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m        (BaseModel): An updated BaseModel object.\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     m = \u001b[38;5;28mself\u001b[39m.model[-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    289\u001b[39m         m, Detect\n\u001b[32m    290\u001b[39m     ):  \u001b[38;5;66;03m# includes all Detect subclasses like Segment, Pose, OBB, WorldDetect, YOLOEDetect, YOLOESegment\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ CELL 6: EVALUATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "metrics = model.val()\n",
    "\n",
    "print(f\"\\nğŸ“Š mAP@50: {metrics.box.map50:.3f}\")\n",
    "print(f\"ğŸ“Š Precision: {metrics.box.mp:.3f}\")\n",
    "print(f\"ğŸ“Š Recall: {metrics.box.mr:.3f}\\n\")\n",
    "\n",
    "for i, name in enumerate(config['names']):\n",
    "    if i < len(metrics.box.ap50):\n",
    "        print(f\"  {name:15s}: {metrics.box.ap50[i]:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "3jcAwI-bYMar"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    @staticmethod\n",
    "    def enhance(img, target_size=(1024,1024)):\n",
    "        # Resize for consistent input\n",
    "        img = cv2.resize(img, target_size)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape)==3 else img\n",
    "\n",
    "        # Denoise & sharpen\n",
    "        gray = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        gray = cv2.filter2D(gray, -1, kernel)\n",
    "\n",
    "        # CLAHE (adaptive histogram equalization)\n",
    "        clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))\n",
    "        gray = clahe.apply(gray)\n",
    "\n",
    "        # Adaptive Threshold for better OCR\n",
    "        return cv2.adaptiveThreshold(gray, 255,\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 199, 45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HCdck3tX1j1"
   },
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"SNKUr0sNWb9nWYv1KO5R\")\n",
    "# project = rf.workspace(\"mywork-f0ue6\").project(\"ids-z3sti\")\n",
    "# version = project.version(4)\n",
    "# dataset = version.download(\"yolov8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "djhJbIk7uY4E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "âœ… PaddleOCR READY\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"NumPy:\", np.__version__)\n",
    "\n",
    "from paddleocr import PaddleOCR\n",
    "ocr = PaddleOCR(lang='ar', use_angle_cls=True, use_gpu=False, show_log=False)\n",
    "\n",
    "print(\"âœ… PaddleOCR READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "tcoh1ZqhYRuR"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1ï¸âƒ£ INSTALL & IMPORT PACKAGES\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q paddleocr paddlepaddle numpy==1.26.4 opencv-python matplotlib arabic-reshaper python-bidi easyocr\n",
    "!apt-get install -q tesseract-ocr tesseract-ocr-ara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "fpDiGG8_kClD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Packages ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2, numpy as np, os, re\n",
    "from paddleocr import PaddleOCR\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… Packages ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "1kflqMfFkRWg"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2ï¸âƒ£ ENHANCED PREPROCESSING CLASS\n",
    "# ============================================================\n",
    "\n",
    "class Preprocessor:\n",
    "    @staticmethod\n",
    "    def enhance(img, aggressive=False):\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape)==3 else img.copy()\n",
    "        h, w = gray.shape[:2]\n",
    "\n",
    "        # Resize small images\n",
    "        if max(h, w) < 200:\n",
    "            scale = 4 if aggressive else 3\n",
    "            gray = cv2.resize(gray, (w*scale, h*scale), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Denoise\n",
    "        gray = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "\n",
    "        # Sharpen\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        gray = cv2.filter2D(gray, -1, kernel)\n",
    "\n",
    "        # CLAHE\n",
    "        clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(16,16))\n",
    "        gray = clahe.apply(gray)\n",
    "\n",
    "        # Adaptive Threshold for ID numbers\n",
    "        if aggressive:\n",
    "            gray = cv2.adaptiveThreshold(gray, 255,\n",
    "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                         cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "        # Add border\n",
    "        gray = cv2.copyMakeBorder(gray, 20, 20, 20, 20,\n",
    "                                  cv2.BORDER_CONSTANT, value=255)\n",
    "        return gray\n",
    "\n",
    "    @staticmethod\n",
    "    def correct_rotation(img):\n",
    "        \"\"\"Detect edges and rotate to correct skew\"\"\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "PgqCL_r9aRVn"
   },
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ProductionOCR:\n",
    "    def __init__(self):\n",
    "        # ØªØ­Ù…ÙŠÙ„ EasyOCR Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù…\n",
    "        self.reader = easyocr.Reader(['ar', 'en'], gpu=False) # ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù€ GPU Ø´ØºØ§Ù„\n",
    "        print(\"âœ… EasyOCR Engine Loaded (Final Fix)\")\n",
    "\n",
    "    def extract(self, img_crop, field_type):\n",
    "        try:\n",
    "            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¹Ø« Ù„Ù€ OCR\n",
    "            gray = cv2.cvtColor(img_crop, cv2.COLOR_BGR2GRAY)\n",
    "            # ØªÙƒØ¨ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©\n",
    "            gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚ÙˆÙ…ÙŠØŒ Ù†Ø³ØªØ®Ø¯Ù… Threshold Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ†\n",
    "            if field_type == 'id_number':\n",
    "                _, gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "            # ØªÙ†ÙÙŠØ° Ø§Ù„Ù€ OCR\n",
    "            results = self.reader.readtext(gray)\n",
    "\n",
    "            if not results:\n",
    "                return {'text': '', 'conf': 0.0}\n",
    "\n",
    "            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Øµ\n",
    "            texts = [res[1] for res in results]\n",
    "            full_text = \" \".join(texts)\n",
    "            conf = np.mean([res[2] for res in results])\n",
    "\n",
    "            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ Clean Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ùƒ\n",
    "            clean_text = self._clean(full_text, field_type)\n",
    "\n",
    "            return {\n",
    "                'text': clean_text,\n",
    "                'conf': float(conf),\n",
    "                'source': 'easyocr_v2'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'text': f'Error: {str(e)}', 'conf': 0.0}\n",
    "\n",
    "    def _clean(self, text, field_type):\n",
    "        # ØªÙ†Ø¸ÙŠÙ Ø¨Ø³ÙŠØ· ÙˆØ³Ø±ÙŠØ¹\n",
    "        if field_type == 'id_number':\n",
    "            digits = re.findall(r'\\d+', text.replace(' ', ''))\n",
    "            res = \"\".join(digits)\n",
    "            return res[-14:] if len(res) >= 14 else res\n",
    "\n",
    "        # Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†ØŒ Ø´ÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù„Ø§ØªÙŠÙ†ÙŠØ© ÙˆØ§Ù„Ø±Ù…ÙˆØ²\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "R1c3ACo_aEJY"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 9: EGYPTIAN ID LOGIC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class IDLogic:\n",
    "    GOVS = {'01':'Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©','02':'Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©','03':'Ø¨ÙˆØ±Ø³Ø¹ÙŠØ¯','04':'Ø§Ù„Ø³ÙˆÙŠØ³',\n",
    "            '11':'Ø¯Ù…ÙŠØ§Ø·','12':'Ø§Ù„Ø¯Ù‚Ù‡Ù„ÙŠØ©','13':'Ø§Ù„Ø´Ø±Ù‚ÙŠØ©','14':'Ø§Ù„Ù‚Ù„ÙŠÙˆØ¨ÙŠØ©',\n",
    "            '15':'ÙƒÙØ± Ø§Ù„Ø´ÙŠØ®','16':'Ø§Ù„ØºØ±Ø¨ÙŠØ©','17':'Ø§Ù„Ù…Ù†ÙˆÙÙŠØ©','18':'Ø§Ù„Ø¨Ø­ÙŠØ±Ø©',\n",
    "            '19':'Ø§Ù„Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„ÙŠØ©','21':'Ø§Ù„Ø¬ÙŠØ²Ø©','22':'Ø¨Ù†ÙŠ Ø³ÙˆÙŠÙ','23':'Ø§Ù„ÙÙŠÙˆÙ…',\n",
    "            '24':'Ø§Ù„Ù…Ù†ÙŠØ§','25':'Ø£Ø³ÙŠÙˆØ·','26':'Ø³ÙˆÙ‡Ø§Ø¬','27':'Ù‚Ù†Ø§','28':'Ø£Ø³ÙˆØ§Ù†',\n",
    "            '29':'Ø§Ù„Ø£Ù‚ØµØ±','31':'Ø§Ù„Ø¨Ø­Ø± Ø§Ù„Ø£Ø­Ù…Ø±','32':'Ø§Ù„ÙˆØ§Ø¯ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯','33':'Ù…Ø·Ø±ÙˆØ­',\n",
    "            '34':'Ø´Ù…Ø§Ù„ Ø³ÙŠÙ†Ø§Ø¡','35':'Ø¬Ù†ÙˆØ¨ Ø³ÙŠÙ†Ø§Ø¡'}\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(id_num):\n",
    "        id_num = re.sub(r'\\D', '', id_num)\n",
    "        if len(id_num) != 14:\n",
    "            return {'valid': False}\n",
    "\n",
    "        try:\n",
    "            century = int(id_num[0])\n",
    "            year = (1900 if century==2 else 2000) + int(id_num[1:3])\n",
    "            month = int(id_num[3:5])\n",
    "            day = int(id_num[5:7])\n",
    "            gov_code = id_num[7:9]\n",
    "            gender = 'Ø°ÙƒØ±' if int(id_num[12])%2==1 else 'Ø£Ù†Ø«Ù‰'\n",
    "            governorate = IDLogic.GOVS.get(gov_code, f'Unknown({gov_code})')\n",
    "            age = datetime.now().year - year\n",
    "\n",
    "            return {\n",
    "                'valid': True,\n",
    "                'birth_date': f\"{day:02d}/{month:02d}/{year}\",\n",
    "                'gender': gender,\n",
    "                'governorate': governorate,\n",
    "                'age': age\n",
    "            }\n",
    "        except:\n",
    "            return {'valid': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "QjLNAbnVayg8"
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"Complete Egyptian ID Processing Pipeline - Optimized Version\"\"\"\n",
    "\n",
    "    def __init__(self, MODEL_PATH):\n",
    "        from ultralytics import YOLO\n",
    "\n",
    "        # 1. ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ YOLO Ø¨ØªØ§Ø¹Ùƒ\n",
    "        self.model = YOLO(MODEL_PATH)\n",
    "\n",
    "        # 2. ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„Ù€ OCR Ø§Ù„Ø¬Ø¯ÙŠØ¯ (EasyOCR/Paddle)\n",
    "        # Ù…Ø¨ÙŠØ­ØªØ§Ø¬Ø´ Arguments Ø¯Ù„ÙˆÙ‚ØªÙŠ Ù„Ø£Ù†Ù‡ Ù…ØªØ¸Ø¨Ø· Ø¬ÙˆÙ‡ Ø§Ù„ÙƒÙ„Ø§Ø³ Ø¨ØªØ§Ø¹Ù‡\n",
    "        self.ocr = ProductionOCR()\n",
    "\n",
    "        # 3. Ø®Ø±ÙŠØ·Ø© Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª (ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¯Ù‡ Ù‡Ùˆ Ø§Ù„Ù„ÙŠ ÙÙŠ Ø§Ù„Ù€ YAML Ø¹Ù†Ø¯Ùƒ)\n",
    "        self.field_map = {\n",
    "            0: 'address',\n",
    "            1: 'birth_date',\n",
    "            2: 'country',\n",
    "            3: 'full_name',\n",
    "            4: 'id_number'\n",
    "        }\n",
    "\n",
    "        print(\"âœ… New Pipeline ready without Google Vision conflicts!\")\n",
    "\n",
    "    def process(self, img_path, visualize=True):\n",
    "        start = datetime.now()\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path) if isinstance(img_path, str) else img_path\n",
    "        if img is None:\n",
    "            return {'success': False, 'error': 'Failed to load image'}\n",
    "\n",
    "        # Detection\n",
    "        results = self.model(img, conf=0.25, verbose=False)\n",
    "\n",
    "        data = {}\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                cid = int(box.cls[0])\n",
    "                det_conf = float(box.conf[0])\n",
    "                field = self.field_map.get(cid)\n",
    "\n",
    "                if not field: continue\n",
    "\n",
    "                # Crop\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                # Ø¥Ø¶Ø§ÙØ© Padding Ø¨Ø³ÙŠØ· Ø¹Ø´Ø§Ù† Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø·Ø±Ø§Ù\n",
    "                h, w = img.shape[:2]\n",
    "                pad = 15\n",
    "                crop = img[max(0,y1-pad):min(h,y2+pad), max(0,x1-pad):min(w,x2+pad)]\n",
    "\n",
    "                # OCR Extraction\n",
    "                ocr_result = self.ocr.extract(crop, field)\n",
    "\n",
    "                data[field] = {\n",
    "                    'text': ocr_result['text'],\n",
    "                    'confidence': min(det_conf, ocr_result['conf']),\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'source': ocr_result.get('source', 'new_engine')\n",
    "                }\n",
    "\n",
    "        # Logic Ù„Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚ÙˆÙ…ÙŠ (ÙÙƒ Ø§Ù„Ø´ÙØ±Ø©)\n",
    "        id_info = None\n",
    "        if 'id_number' in data and len(data['id_number']['text']) >= 14:\n",
    "            id_info = IDLogic.parse(data['id_number']['text'])\n",
    "            if id_info.get('valid'):\n",
    "                # ØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚ÙˆÙ…ÙŠ\n",
    "                data['birth_date_from_id'] = {'text': id_info['birth_date'], 'confidence': 1.0}\n",
    "                data['governorate'] = {'text': id_info['governorate'], 'confidence': 1.0}\n",
    "                data['gender'] = {'text': id_info['gender'], 'confidence': 1.0}\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'time': (datetime.now() - start).total_seconds(),\n",
    "            'fields': data,\n",
    "            'id_logic': id_info\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "uy4hMmbOYVB1"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 5: DISPLAY RESULTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def show_result(result):\n",
    "    \"\"\"Display extraction results\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ†” Ù†ØªØ§Ø¦Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø·Ø§Ù‚Ø©\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if not result['success']:\n",
    "        print(f\"âŒ Ø®Ø·Ø£: {result.get('error')}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nâ±ï¸  Ø§Ù„ÙˆÙ‚Øª: {result['time']:.2f} Ø«Ø§Ù†ÙŠØ©\\n\")\n",
    "\n",
    "    print(\"ğŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    fields_ar = {\n",
    "        'full_name': 'Ø§Ù„Ø§Ø³Ù…',\n",
    "        'id_number': 'Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚ÙˆÙ…ÙŠ',\n",
    "        'birth_date': 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯',\n",
    "        'address': 'Ø§Ù„Ø¹Ù†ÙˆØ§Ù†',\n",
    "        'country': 'Ø§Ù„Ø¬Ù†Ø³ÙŠØ©'\n",
    "    }\n",
    "\n",
    "    for field in ['full_name', 'id_number', 'birth_date', 'address', 'country']:\n",
    "        if field in result['fields']:\n",
    "            d = result['fields'][field]\n",
    "            ar_name = fields_ar[field]\n",
    "            print(f\"  {ar_name:15s}: {d['text']}\")\n",
    "            print(f\"  {'':15s}  Ø§Ù„Ø«Ù‚Ø©: {d['confidence']:.1%} | Ø§Ù„Ù…ØµØ¯Ø±: {d.get('source', 'N/A')}\")\n",
    "\n",
    "    if result.get('id_logic') and result['id_logic'].get('valid'):\n",
    "        print(\"\\nğŸ§  Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚ÙˆÙ…ÙŠ:\")\n",
    "        print(\"-\" * 80)\n",
    "        info = result['id_logic']\n",
    "        print(f\"  ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯: {info['birth_date']}\")\n",
    "        print(f\"  Ø§Ù„Ø¹Ù…Ø±: {info['age']} Ø³Ù†Ø©\")\n",
    "        print(f\"  Ø§Ù„Ù†ÙˆØ¹: {info['gender']}\")\n",
    "        print(f\"  Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©: {info['governorate']}\")\n",
    "\n",
    "    if result.get('viz_path'):\n",
    "        print(f\"\\nğŸ’¾ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {result['viz_path']}\")\n",
    "\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            img = cv2.imread(result['viz_path'])\n",
    "            if img is not None:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "                plt.title('Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒØ´Ù')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "NWvWKvNbYkjx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Ø¬Ø§Ù‡Ø²! Ø´ØºÙ„ÙŠ: upload_and_process()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "def upload_and_process():\n",
    "    \"\"\"Ø§Ø®ØªÙŠØ§Ø± ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ±Ø© Ø§Ù„Ø¨Ø·Ø§Ù‚Ø© Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø²Ùƒ Ø§Ù„Ù…Ø­Ù„ÙŠ\"\"\"\n",
    "    # Ø¥Ø®ÙØ§Ø¡ Ù†Ø§ÙØ°Ø© Tkinter Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    print(\"ğŸ“¤ Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ø§Ù„Ø¨Ø·Ø§Ù‚Ø©:\")\n",
    "    fname = askopenfilename(\n",
    "        title=\"Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ø§Ù„Ø¨Ø·Ø§Ù‚Ø©\",\n",
    "        filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp\")]\n",
    "    )\n",
    "\n",
    "    if not fname:\n",
    "        print(\"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ ØµÙˆØ±Ø©!\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nâœ… Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {os.path.basename(fname)}\\n\")\n",
    "\n",
    "    # Ù†ÙØªØ±Ø¶ pipe Ù‡Ùˆ Pipeline Ø§Ù„Ù„ÙŠ Ø¹Ø§Ù…Ù„ØªÙ‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§\n",
    "    result = pipe.process(fname)\n",
    "    \n",
    "    # ØªØ§Ø¨Ø¹ Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "    show_result(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"\\nğŸš€ Ø¬Ø§Ù‡Ø²! Ø´ØºÙ„ÙŠ: upload_and_process()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "37op8EEWdXDn"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ù‡Ù†Ø§ Ø¨Ù†Ø¨Ø¹Øª Ø§Ù„Ù€ MODEL_PATH Ø¨Ø³ØŒ ÙˆÙ…ÙÙŠØ´ google_credentials Ø®Ø§Ù„Øµ\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pipe = \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø±ÙØ¹ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\u001b[39;00m\n\u001b[32m      5\u001b[39m upload_and_process()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, MODEL_PATH)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 1. ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ YOLO Ø¨ØªØ§Ø¹Ùƒ\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2. ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„Ù€ OCR Ø§Ù„Ø¬Ø¯ÙŠØ¯ (EasyOCR/Paddle)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Ù…Ø¨ÙŠØ­ØªØ§Ø¬Ø´ Arguments Ø¯Ù„ÙˆÙ‚ØªÙŠ Ù„Ø£Ù†Ù‡ Ù…ØªØ¸Ø¨Ø· Ø¬ÙˆÙ‡ Ø§Ù„ÙƒÙ„Ø§Ø³ Ø¨ØªØ§Ø¹Ù‡\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.ocr = ProductionOCR()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/models/yolo/model.py:76\u001b[39m, in \u001b[36mYOLO.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m = new_instance.\u001b[34m__dict__\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRTDETR\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model[-\u001b[32m1\u001b[39m]._get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:144\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mself\u001b[39m._new(model, task=task, verbose=verbose)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:283\u001b[39m, in \u001b[36mModel._load\u001b[39m\u001b[34m(self, weights, task)\u001b[39m\n\u001b[32m    280\u001b[39m weights = checks.check_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo26 -> yolo26n.pt\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights).rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.ckpt = \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m.task = \u001b[38;5;28mself\u001b[39m.model.task\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m.overrides = \u001b[38;5;28mself\u001b[39m.model.args = \u001b[38;5;28mself\u001b[39m._reset_ckpt_args(\u001b[38;5;28mself\u001b[39m.model.args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:1489\u001b[39m, in \u001b[36mload_checkpoint\u001b[39m\u001b[34m(weight, device, inplace, fuse)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_checkpoint\u001b[39m(weight, device=\u001b[38;5;28;01mNone\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m, fuse=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1477\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a single model weights.\u001b[39;00m\n\u001b[32m   1478\u001b[39m \n\u001b[32m   1479\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1487\u001b[39m \u001b[33;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[32m   1488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1489\u001b[39m     ckpt, weight = \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1490\u001b[39m     args = {**DEFAULT_CFG_DICT, **(ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[32m   1491\u001b[39m     model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:1437\u001b[39m, in \u001b[36mtorch_safe_load\u001b[39m\u001b[34m(weight, safe_only)\u001b[39m\n\u001b[32m   1435\u001b[39m                 ckpt = torch_load(f, pickle_module=safe_pickle)\n\u001b[32m   1436\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m             ckpt = \u001b[43mtorch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.name == \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/ultralytics/utils/patches.py:158\u001b[39m, in \u001b[36mtorch_load\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    156\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/OCR_IDS/OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/venv/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'OCR_System_for_National_ID_Cards-Egyptian-Kuwaiti-/models/best.pt'"
     ]
    }
   ],
   "source": [
    "# Ù‡Ù†Ø§ Ø¨Ù†Ø¨Ø¹Øª Ø§Ù„Ù€ MODEL_PATH Ø¨Ø³ØŒ ÙˆÙ…ÙÙŠØ´ google_credentials Ø®Ø§Ù„Øµ\n",
    "pipe = Pipeline(MODEL_PATH)\n",
    "\n",
    "# Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø±ÙØ¹ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
    "upload_and_process()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
